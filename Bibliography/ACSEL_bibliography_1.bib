% Encoding: UTF-8

@InProceedings{bousquet2008tradeoffs,
  author    = {Bousquet, Olivier and Bottou, L{\'e}on},
  title     = {The tradeoffs of large scale learning},
  booktitle = {Advances in neural information processing systems},
  year      = {2008},
  pages     = {161--168},
  file      = {:bousquet2008tradeoffs.pdf:PDF},
  keywords  = {rank2},
  review    = {hard},
}

@Article{Courbariaux2014,
  author        = {Matthieu Courbariaux and Yoshua Bengio and Jean-Pierre David},
  title         = {Training deep neural networks with low precision multiplications},
  journal       = {arXiv preprint arXiv:1412.7024},
  year          = {2014},
  __markedentry = {[kito:1]},
  abstract      = {Multipliers are the most space and power-hungry arithmetic operators of the digital implementation of deep neural networks. We train a set of state-of-the-art neural networks (Maxout networks) on three benchmark datasets: MNIST, CIFAR-10 and SVHN. They are trained with three distinct formats: floating point, fixed point and dynamic fixed point. For each of those datasets and for each of those formats, we assess the impact of the precision of the multiplications on the final error after training. We find that very low precision is sufficient not just for running trained networks but also for training them. For example, it is possible to train Maxout networks with 10 bits multiplications.},
  date          = {2014-12-22},
  eprint        = {1412.7024v5},
  eprintclass   = {cs.LG},
  eprinttype    = {arXiv},
  file          = {online:http\://arxiv.org/pdf/1412.7024v5:PDF},
  keywords      = {cs.LG, cs.CV, cs.NE, rank3},
}

@Unpublished{denis:hal-01192668,
  author        = {Denis, Christophe and de Oliveira Castro, Pablo and Petit, Eric},
  title         = {{Verificarlo: checking floating point accuracy through Monte Carlo Arithmetic}},
  note          = {working paper or preprint},
  month         = Sep,
  year          = {2015},
  __markedentry = {[kito:3]},
  doi           = {10.1109/arith.2016.31},
  file          = {:denis_hal-01192668 - Verificarlo_ checking floating point accuracy through Monte Carlo Arithmetic.pdf:PDF;verificarlo-preprint.pdf:https\://hal.archives-ouvertes.fr/hal-01192668/file/verificarlo-preprint.pdf:PDF},
  hal_id        = {hal-01192668},
  hal_version   = {v3},
  keywords      = {floating point arithmetic ; compilers ; numerical analysis ; Monte Carlo arithmetic, rank5},
  url           = {https://hal.archives-ouvertes.fr/hal-01192668},
}

@Article{Goldberg1991,
  author        = {David Goldberg},
  title         = {What Every Computer Scientist Should Know About Floating-Point Arithmetic},
  journal       = {{ACM Computing Surveys}},
  year          = {1991},
  volume        = {23},
  number        = {1},
  pages         = {5--48},
  month         = mar,
  issn          = {0360-0300},
  __markedentry = {[kito:4]},
  abstract      = {Floating-point arithmetic is considered an esoteric
 subject by many peopl. This is rather surprising,
 because floating-point is ubiquitous in computer
 systems: Almost every language has a floating-point
 datatype; computers from PCs to supercomputers have
 floating-point accelarators; most compilers will be
 called upon to compile floating-point alogrothms from
 time to time; and virtually every operating system must
 respond to floating-point exceptions such as overflow.
 This paper presents a tutorial on the aspects of
 floating-point that have a direct impact on designers
 of computer systems. It begins with background on
 floating-point represtentation and rounding error,
 continues with a discussion of the IEEE floating-point
 standard, and concludes with examples of how computer
 system builders can better support floating point},
  added-by      = {aso},
  doi           = {10.1145/103162.103163},
  file          = {:Goldberg1991 - What Every Computer Scientist Should Know About Floating-Point Arithmetic.pdf:PDF},
  keywords      = {Algorithms, Design, Languages, denormalized number, exception, floating-point, floating-point standard, gradual underflow, guard digit, NaN, overflow, relative error, rounding error, rounding mode, ulp, underflow, rank5},
  url           = {http://www.lsi.upc.edu/~robert/teaching/master/material/p5-goldberg.pdf},
}

@InProceedings{Gupta2015,
  author        = {Gupta, Suyog and Agrawal, Ankur and Gopalakrishnan, Kailash and Narayanan, Pritish},
  title         = {Deep learning with limited numerical precision},
  booktitle     = {Proceedings of the 32nd International Conference on Machine Learning (ICML-15)},
  year          = {2015},
  pages         = {1737--1746},
  __markedentry = {[kito:2]},
  file          = {:Gupta2015 - Deep learning with limited numerical precision.pdf:PDF},
  keywords      = {rank4},
  review        = {# Objectif
Leur objectif ici est de travailler sur des réseaux de neurones avec une précision limitée ; en l'occurence, avec une arithmétique à virgule fixe.
# Motivations
Virgule fixe car consomme moins/est plus rapide/plus léger en mémoire...
# Moyens
Jeux de données MNIST et CIFAR10 ( petit jeu de donnée )
# Résultats
Leurs résultats semblent montrer que des nombres représentes avec **16 bits** de largeur seulement suffisent (en utilisant un *arrondi stochastique*).
Ils exploitent la tolérance au bruit des réseaux de neurones.
** Ils arrivent à des résultats similaires à un entrainement fait par de l'arithmétique flottante à 32 bits avec de la fixed point à faible précision *grace au stochastic rounding*!**
# A creuser
* Ils auraient déterminé que le mode d'arrondi jouait un rôle crucial durant l'entraînement ^[a creuser]^.
* Note : Des erreurs de computations peuvent elles vraiment être interchangeables avec du bruit? (quid de la non linéarité?)
* Extrapoler leur figure 1 avec des réseaux de neurones plus badass / grands
# Notes
* L'efficacité de méthodes d'apprentissage profond dépend de la capacité du hardware sous-jacent à exécuter un entraînement supervisé rapide de réseaux complexes en utilisant de grandes quantités de données labellisées.
* Les réseaux de neurones ont une résilience à l'erreur assez forte ; la précision n'est pas si importante (on en rajoute même volontairement de manière non linéaire avec du bruit). La plupart des hardware utilisent une forte précision, là où ici ça serait pas nécessaire ^[référence nécessaire]^
* Le stochastic rounding semble indispensable, car en "round to nearest" ils divergent presque à chaque fois avec une faible précision ; nécessité donc d'utiliser le stochastique rounding (note pour mon article : faire une partie sur le choix du mode d'arrondi, et une autre sur le choix de la précision ; avoir un diagramme mettant tout cela en parallèle (4D, séries de diagrammes en 3D) serait top)},
  url           = {http://proceedings.mlr.press/v37/gupta15.pdf},
}

@Article{TapenadeRef13,
  author        = {Hasco{\"e}t, L. and Pascual, V.},
  title         = {The {T}apenade {A}utomatic {D}ifferentiation tool: {P}rinciples, {M}odel, and {S}pecification},
  journal       = {{ACM} {T}ransactions {O}n {M}athematical {S}oftware},
  year          = {2013},
  volume        = {39},
  number        = {3},
  __markedentry = {[kito:1]},
  comment       = {http://www-sop.inria.fr/tropics/tapenade.html},
  file          = {:TapenadeRef13 - The Tapenade Automatic Differentiation tool_ Principles, Model, and Specification.pdf:PDF},
  keywords      = {rank3},
  url           = {http://dx.doi.org/10.1145/2450153.2450158},
}

@Article{holi1993finite,
  author        = {Holi, Jordan L and Hwang, J-N},
  title         = {Finite precision error analysis of neural network hardware implementations},
  journal       = {IEEE Transactions on Computers},
  year          = {1993},
  volume        = {42},
  number        = {3},
  pages         = {281--290},
  __markedentry = {[kito:1]},
  doi           = {10.1109/12.210171},
  file          = {:autres/holi1993.pdf:PDF},
  keywords      = {rank2},
  publisher     = {IEEE},
  review        = {Ils analysent l'impact de la précision (fixed point) sur le réseau de neurone :o
Demander l'avis de David},
}

@PhdThesis{HOUEDANOU2015,
  author      = {HOUEDANOU, Wilfrid Koffi},
  title       = {{A posteriori error estimates of some finites elements methods for the Stokes-Darcy coupled problem : isotropic and anisotropic discretizations}},
  school      = {{Institut de Math{\'e}matiques et de Sciences Physiques (IMSP)}},
  year        = {2015},
  type        = {Theses},
  month       = Dec,
  file        = {:HOUEDANOU2015 - A posteriori error estimates of some finites elements methods for the Stokes-Darcy coupled problem _ isotropic and anisotropic discretizations.pdf:PDF;These.pdf:https\://hal.archives-ouvertes.fr/tel-01373344/file/These.pdf:PDF},
  hal_id      = {tel-01373344},
  hal_version = {v1},
  keywords    = {Coupled problem Stokes-Darcy ; A posteriori error estimates ; Finites elements Method ; Probl{\`e}me de transmission Stokes-Darcy ; Analyse d'erreur a-posteriori ; M{\'e}thode d'{\'e}l{\'e}ments finis., rank2},
  url         = {https://hal.archives-ouvertes.fr/tel-01373344},
}

@InCollection{Hubara2016,
  author    = {Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
  title     = {Binarized Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems 29},
  publisher = {Curran Associates, Inc.},
  year      = {2016},
  editor    = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
  pages     = {4107--4115},
  file      = {:Hubara2016 - Binarized Neural Networks.pdf:PDF},
  keywords  = {rank2},
  url       = {http://papers.nips.cc/paper/6573-binarized-neural-networks.pdf},
}

@Article{Kahl2017,
  author    = {K. Kahl and H. Rittich},
  title     = {The deflated conjugate gradient method: Convergence, perturbation and accuracy},
  journal   = {Linear Algebra and its Applications},
  year      = {2017},
  volume    = {515},
  number    = {Supplement C},
  pages     = {111 - 129},
  issn      = {0024-3795},
  abstract  = {Deflation techniques for Krylov subspace methods have seen a lot of attention in recent years. They provide means to improve the convergence speed of these methods by enriching the Krylov subspace with a deflation subspace. The most common approach for the construction of deflation subspaces is to use (approximate) eigenvectors, but also more general subspaces are applicable.

In this paper we discuss two results concerning the accuracy requirements within the deflated CG method. First we show that the effective condition number which bounds the convergence rate of the deflated conjugate gradient method depends asymptotically linearly on the size of the perturbations in the deflation subspace. Second, we discuss the accuracy required in calculating the deflating projection. This is crucial concerning the overall convergence of the method, and also allows to save some computational work.

To show these results, we use the fact that as a projection approach deflation has many similarities to multigrid methods. In particular, recent results relate the spectra of the deflated matrix to the spectra of the error propagator of twogrid methods. In the spirit of these results we show that the effective condition number can be bounded by the constant of a weak approximation property.},
  doi       = {10.1016/j.laa.2016.10.027},
  file      = {:16 - The deflated conjugate gradient method \: convergence, perturbation and accuracy.pdf:PDF},
  keywords  = {Conjugate gradients, Deflation, Multigrid, Convergence, Perturbation, rank1},
  owner     = {kito},
  timestamp = {2017.11.24},
  url       = {http://www.sciencedirect.com/science/article/pii/S0024379516305122},
}

@Article{Koester2017,
  author         = {Urs Köster and Tristan Webb and Xin Wang and Marcel Nassar and Arjun Bansal and William Constable and Oguz Elibol and Stewart Hall and Luke Hornof and Amir Khosrowshahi and Carey Kloss and Ruby Pai and Naveen Rao},
  title          = {Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks},
  year           = {2017},
  month          = nov,
  __markedentry  = {[kito:1]},
  abstract       = {Deep neural networks are commonly developed and trained in 32-bit floating point format. Significant gains in performance and energy efficiency could be realized by training and inference in numerical formats optimized for deep learning. Despite advances in limited precision inference in recent years, training of neural networks in low bit-width remains a challenging problem. Here we present the Flexpoint data format, aiming at a complete replacement of 32-bit floating point format training and inference, designed to support modern deep network topologies without modifications. Flexpoint tensors have a shared exponent that is dynamically adjusted to minimize overflows and maximize available dynamic range. We validate Flexpoint by training AlexNet, a deep residual network and a generative adversarial network, using a simulator implemented with the neon deep learning framework. We demonstrate that 16-bit Flexpoint closely matches 32-bit floating point in training all three models, without any need for tuning of model hyperparameters. Our results suggest Flexpoint as a promising numerical format for future hardware for training and inference.},
  comments       = {14 pages, 5 figures, accepted in Neural Information Processing Systems 2017},
  eprint         = {1711.02213},
  file           = {:Koester2017.pdf:PDF},
  keywords       = {rank3},
  oai2identifier = {1711.02213},
  owner          = {kito},
  review         = {Entrainement de réseaux de neurones avec un nouveau format qui se veut plus efficace (où 16 bits semblent suffire).
A creuser},
  timestamp      = {2017.11.24},
  url            = {https://arxiv.org/abs/1711.02213},
}

@InProceedings{Rubio-Gonzalez2013,
  author        = {Cindy Rubio-Gonz{\'{a}}lez and Cuong Nguyen and Hong Diep Nguyen and James Demmel and William Kahan and Koushik Sen and David H. Bailey and Costin Iancu and David Hough},
  title         = {Precimonious},
  booktitle     = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis on - {SC} {\textquotesingle}13},
  year          = {2013},
  publisher     = {{ACM} Press},
  __markedentry = {[kito:1]},
  doi           = {10.1145/2503210.2503296},
  file          = {:Rubio-Gonzalez2013 - Precimonious.pdf:PDF},
  keywords      = {rank4},
}

@Article{Springenberg2014,
  author      = {Jost Tobias Springenberg and Alexey Dosovitskiy and Thomas Brox and Martin Riedmiller},
  title       = {Striving for Simplicity: The All Convolutional Net},
  journal     = {arXiv preprint arXiv:1412.6806},
  year        = {2014},
  abstract    = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
  date        = {2014-12-21},
  eprint      = {1412.6806v3},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1412.6806v3:PDF},
  keywords    = {cs.LG, cs.CV, cs.NE, rank1},
  review      = {# Motivations
La plupart des CNN utilisés pour la reconnaissance d'objets sont constitués de la même façon : alterner des couches de convolutions et de max-pooling, suivis d'un petit nombre de couches entièrement connectées.
# Objectifs
S'interroger sur l'utilité de cette pipeline.
# Résultats
* Le pattern *max-pooling* peut être remplacé par une couche convolutionnelle avec un pas plus grand sans perte de précision sur plusieurs benchmarks de reconnaissance d'images. ( efficacité proche ou state-of-the-art).
* Ils proposent une nouvelle architecture constituée uniquement de couche convolutives.
* Ils ont même un moyen d'analyser et de visualiser le réseau, basé sur une variante de "l'approche déconvolutive" ^[référence nécessaire]^.
* Inclure des opérations de (max-)pooling explicites n'améliore *pas toujours* la performance du CNN.
#Notes
Approche intéressante ; leur but est humblement de déterminer ce dont on a besoin *au minimum*, et de sortir un peu des sentiers battus usés et réusés de conv conv max pool conv conv max pool etc.
Non directement corrélé à mon sujet.},
}

@Book{Parker1997,
  title         = {Monte Carlo Arithmetic: exploiting randomness in floating-point arithmetic},
  publisher     = {University of California (Los Angeles). Computer Science Department},
  year          = {1997},
  author        = {Parker, Douglass Stott},
  __markedentry = {[kito:4]},
  file          = {:parker1997monte - Monte Carlo Arithmetic_ exploiting randomness in floating-point arithmetic.pdf:PDF},
  keywords      = {rank5},
  url           = {http://web.cs.ucla.edu/~stott/mca/},
}

@Article{graillat2011stochastic,
  author        = {Graillat, Stef and J{\'e}z{\'e}quel, Fabienne and Wang, Shiyue and Zhu, Yuxiang},
  title         = {Stochastic arithmetic in multiprecision},
  journal       = {Mathematics in Computer Science},
  year          = {2011},
  volume        = {5},
  number        = {4},
  pages         = {359--375},
  __markedentry = {[kito:2]},
  doi           = {10.1007/s11786-011-0103-4},
  file          = {:graillat2011stochastic - Stochastic arithmetic in multiprecision.pdf:PDF},
  keywords      = {rank3},
  publisher     = {Springer},
  review        = {CADNA with MPFR},
}

@Misc{shewchuk1994introduction,
  author        = {Shewchuk, Jonathan Richard and others},
  title         = {An introduction to the conjugate gradient method without the agonizing pain},
  year          = {1994},
  __markedentry = {[kito:1]},
  keywords      = {rank2},
  publisher     = {Carnegie-Mellon University. Department of Computer Science},
}

@Article{aviat2016truncated,
  author    = {Aviat, Félix and Levitt, Antoine and Stamm, Benjamin and Maday, Yvon and Ren, Pengyu and Ponder, Jay W and Lagardère, Louis and Piquemal, Jean-Philip},
  title     = {Truncated Conjugate Gradient: An Optimal Strategy for the Analytical Evaluation of the Many-Body Polarization Energy and Forces in Molecular Simulations},
  journal   = {Journal of chemical theory and computation},
  year      = {2016},
  volume    = {13},
  number    = {1},
  pages     = {180--190},
  doi       = {10.1021/acs.jctc.6b00981},
  file      = {:aviat2016truncated - Truncated Conjugate Gradient_ An Optimal Strategy for the Analytical Evaluation of the Many-Body Polarization Energy and Forces in Molecular Simulations.6b00981:},
  keywords  = {rank1},
  publisher = {ACS Publications},
}

@Book{Parker1997a,
  title         = {Monte Carlo Arithmetic: a framework for the statistical analysis of roundoff error},
  publisher     = {University of California (Los Angeles). Computer Science Department},
  year          = {1997},
  author        = {Parker, Douglass Stott and Eggert, Paul Richard and Pierce, Brad},
  __markedentry = {[kito:4]},
  file          = {:parker1997monte - Monte Carlo Arithmetic_ a framework for the statistical analysis of roundoff error.pdf:PDF},
  keywords      = {floating-point arithmetic, floating-point rounding, roundoff error, random rounding, ANSI/IEEE floating-point standards, significance arithmetic, Monte Carlo methods, rank5},
}

@Book{Hazewinkel1994,
  title     = {Numerical analysis.},
  publisher = {Springer Science+Business Media B.V. / Kluwer Academic Publishers},
  year      = {1994},
  author    = {Hazewinkel, Michiel},
  edition   = {2001},
  isbn      = {978-1-55608-010-4},
  booktitle = {Encyclopedia of Mathematics.},
  keywords  = {rank3},
  maintitle = {Encyclopedia of Mathematics.},
  url       = {http://www.encyclopediaofmath.org/index.php?title=Numerical_analysis&oldid=36393},
}

@Article{zuras2008ieee,
  author        = {Zuras, Dan and Cowlishaw, Mike and Aiken, Alex and Applegate, Matthew and Bailey, David and Bass, Steve and Bhandarkar, Dileep and Bhat, Mahesh and Bindel, David and Boldo, Sylvie and others},
  title         = {IEEE standard for floating-point arithmetic},
  journal       = {IEEE Std 754-2008},
  year          = {2008},
  pages         = {1--70},
  __markedentry = {[kito:1]},
  doi           = {10.1109/IEEESTD.2008.4610935},
  isbn          = {978-0-7381-5752-8},
  keywords      = {rank4},
  publisher     = {IEEE},
}

@Article{kaneko1973local,
  author    = {Kaneko, Toyohisa and Liu, Bede},
  title     = {On local roundoff errors in floating-point arithmetic},
  journal   = {Journal of the ACM (JACM)},
  year      = {1973},
  volume    = {20},
  number    = {3},
  pages     = {391--398},
  publisher = {ACM},
}

@Article{fevotte2016verrou,
  author = {F{\'e}votte, Fran{\c{c}}ois and Lathuili{\`e}re, Bruno},
  title  = {VERROU: Assessing Floating-Point Accuracy Without Recompiling},
  year   = {2016},
}

@Comment{jabref-meta: databaseType:bibtex;}
